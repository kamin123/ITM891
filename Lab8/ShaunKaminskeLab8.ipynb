{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data\n",
        "\n",
        "1. Load mnist_test.csv from https://www.kaggle.com/datasets/oddrationale/mnist-in-csv?select=mnist_test.csv as data.\n",
        "\n",
        "2. Split data into X and y. X should have the shape as (10000,784) and y should have the shape as (10000,1).\n",
        "\n",
        "3. Split X and y into the train set (80%) and the test set (20%). The train set is for fitting your model while the test set is for evaluating your model. As a result, you will have X_train.shape as (8000,784), y_train.shape as (8000,1), X_test.shape as (2000,784),and y_test.shape as (2000,1). (Hint: use sklearn.model_selection.train_test_split.) "
      ],
      "metadata": {
        "id": "qtt-crrf_Kth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add your code to this \n",
        "\n",
        "data_pd = pd.read_csv('/content/mnist_test.csv')\n",
        "data = np.array(data_pd)\n",
        "\n",
        "X = data[:, 1:]\n",
        "print(X.shape)\n",
        "y = data[:, 0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "OS8Q_A0PB10T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ca77eb-7f9b-4ebe-acbc-7a414c12973e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 784)\n",
            "(8000, 784)\n",
            "(2000, 784)\n",
            "(8000,)\n",
            "(2000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting and Evaluating Your Model\n",
        "1. Use sklearn.linear_model.RidgeClassifier to fit X_train and y_train to get a multi-class classification model. \n",
        "2. Test your model on (X_test, y_test) and get testing accuracy by using clf.score(X_test, y_test) assuming your model is named as \"clf\"."
      ],
      "metadata": {
        "id": "ZfyYzOYAB5gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "clf = RidgeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "acc = clf.score(X_test, y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "vG6jxQQYefDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8a87f0-f0fa-4043-fb11-8bf3465a5d24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing the RidgeClassifier\n",
        "1. In sklearn.linear_model.RidgeClassifier, there is one argument called \"alpha\" corresponding to the coefficient for the regularization. By default, alpha is equal to 1. There are benefits and drawbacks to having a large alpha. The larger is the alpha, the more likely you are going to have underfitting problems with your graph. Higher alpha does not necessarily mean better results. On the other hand, a low alpha may lead to overfitting problems and a more complicated model. \n",
        "\n",
        "  More information: https://towardsdatascience.com/preventing-overfitting-with-lasso-ridge-and-elastic-net-regularization-in-machine-learning-d1799b05d382\n",
        "\n",
        "  Please try different alpha to train your model and evaluate your model's test accuracy. Note: you cannot try number ranges such as (1-10) or (1-50), these numbers are too similar. (Hint: you'll want try alphas that are different powers of 10.) Out of what you chose, what is the best choice for alpha in MNIST classification?\n",
        "\n",
        "2. Instead of fitting the full dimension (784) of data to the RidgeClassifier, you can apply PCA to the data (PCA over X with the shape 10000*784) to reduce the dimension from 784 to 100 (for example) and train another RidgeClassifier with 100-dimension features. \n",
        "\n",
        "  Typically, we want the explained variance to be between 95–99% (which is what we would set n_components to). With alpha=1, iterate through the array 0.95-0.99 ([0.95, 0.96, ...., 0.99]) and set n_components to the value you're currently on in the array. Each time, print the shape of X_reduced to get the number of components that are left from the second value in the tuple. For example, (10000, 168) has 168 components left. For further explanation, see the scikit learn PCA function documentation: \n",
        "  > if 0 < n_components < 1 and svd_solver == ‘full’, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components\n",
        " \n",
        "  Then, set n_components to 784 and run again. I should see the results from 0.95-0.99 variance as well as the result of running n_components = 784 in your answer. Then answer this question: what is the best reduced dimension number of components to get the highest test accuracy? (Hint: after applying PCA to 10000 samples, remember to split train and test.)\n",
        "  \n",
        "  More information on variance and PCA:  https://stackoverflow.com/questions/32857029/python-scikit-learn-pca-explained-variance-ratio-cutoff\n"
      ],
      "metadata": {
        "id": "1W1rxcCZegU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 \n",
        "# Add your code to this \n",
        "acc_best = 0\n",
        "for alpha in [.0001, .001, .01, .1, 1, 10, 100]: \n",
        "  clf = RidgeClassifier(alpha = alpha)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  acc = clf.score(X_test, y_test)\n",
        "  print(f'{alpha} has accuracy score of: {acc}')\n",
        "  \n",
        "  \n",
        "  if acc > acc_best:\n",
        "    acc_best = acc\n",
        "    alpha_best = alpha\n",
        "  \n",
        "\n",
        "print(f\"\\n\\nbest accuracy is {acc_best} with alpha: {alpha_best}\\n\\n\")\n",
        "\n",
        "\n",
        "# 2 \n",
        "# Add your code to this \n",
        "\n",
        "acc_best_PCA = 0\n",
        "from sklearn.decomposition import PCA\n",
        "for i in np.linspace(.95, .99, 5):\n",
        "    print(i)\n",
        "    my_model = PCA(n_components= i, svd_solver = 'full')\n",
        "    x_reduced = my_model.fit_transform(X)\n",
        "    print(x_reduced.shape)\n",
        "    y = data[:, 0]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x_reduced, y, test_size = 0.2)\n",
        "    clf = RidgeClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    acc_with_PCA_reduced = clf.score(X_test, y_test)\n",
        "    print(f'Accuracy is {acc_with_PCA_reduced} with {x_reduced.shape[1]} columns\\n')\n",
        "\n",
        "    if acc_with_PCA_reduced > acc_best_PCA:\n",
        "      acc_best_PCA = acc_with_PCA_reduced\n",
        "      PCA_best = x_reduced.shape[1]\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "my_model = PCA(n_components= 784, svd_solver= 'full')\n",
        "X_PCA = my_model.fit_transform(X)\n",
        "print(my_model.n_components)\n",
        "print(X_PCA.shape)\n",
        "y = data[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_PCA, y, test_size = 0.2)\n",
        "clf = RidgeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "acc_with_PCA_784 = clf.score(X_test, y_test)\n",
        "print(f'Best accuracy is {acc_with_PCA_784} with {X_PCA.shape[1]} columns')\n",
        "\n",
        "\n",
        "print(f'\\n\\n\\n\\nThe best reduced dimension number of components to get the highest test accuracy is {PCA_best} components returning a test accuracy of {acc_best_PCA}')\n",
        "\n",
        " \n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "2OK0bA4IrY47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b1e827-580b-457f-ce0e-a3f2f9fb244b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0001 has accuracy score of: 0.8495\n",
            "0.001 has accuracy score of: 0.8495\n",
            "0.01 has accuracy score of: 0.85\n",
            "0.1 has accuracy score of: 0.8495\n",
            "1 has accuracy score of: 0.851\n",
            "10 has accuracy score of: 0.8515\n",
            "100 has accuracy score of: 0.8515\n",
            "\n",
            "\n",
            "best accuracy is 0.8515 with alpha: 10\n",
            "\n",
            "\n",
            "0.95\n",
            "(10000, 149)\n",
            "Accuracy is 0.8435 with 149 columns\n",
            "\n",
            "0.96\n",
            "(10000, 174)\n",
            "Accuracy is 0.857 with 174 columns\n",
            "\n",
            "0.97\n",
            "(10000, 207)\n",
            "Accuracy is 0.8645 with 207 columns\n",
            "\n",
            "0.98\n",
            "(10000, 253)\n",
            "Accuracy is 0.8535 with 253 columns\n",
            "\n",
            "0.99\n",
            "(10000, 323)\n",
            "Accuracy is 0.855 with 323 columns\n",
            "\n",
            "784\n",
            "(10000, 784)\n",
            "Best accuracy is 0.8345 with 784 columns\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The best reduced dimension number of components to get the highest test accuracy is 207 components returning a test accuracy of 0.8645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix and Classification Report\n",
        "1. Read https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html to understand how to use a confusion matrix. Based on the information you learned from #1, can you plot the confusion matrix accordingly? (Hint: use clf.predict(X_test) to get the prediction labels over X_test.)\n",
        "\n",
        "2. Read https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report to understand how to use a classification report. Based on the information you learned in #3, can you output the classification report accordingly? What is the label with the lowest precision?"
      ],
      "metadata": {
        "id": "mitvXX17remR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Add your code here\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print('The Label with the lowest precision is 4')"
      ],
      "metadata": {
        "id": "FxhAoKrTrgLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7fb7fb8-8ea8-4d62-ed58-9547db5d1e0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[166   0   0   1   2   2   1   0   3   0]\n",
            " [  0 216   0   0   0   0   0   0   3   0]\n",
            " [  5  13 178   8   4   0   8   9  10   1]\n",
            " [  2   2   3 164   2   7   2   6   6   1]\n",
            " [  0   2   3   1 170   2   3   0   1   6]\n",
            " [  6   1   1  18   5 141   4   1  10   3]\n",
            " [  3   1   4   1   2   4 161   0   0   0]\n",
            " [  0  11   4   0   6   0   1 183   1   9]\n",
            " [  3  12   3   9   3  12   1   3 147  12]\n",
            " [  4   3   2   3  20   0   1  23   2 143]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.91       175\n",
            "           1       0.83      0.99      0.90       219\n",
            "           2       0.90      0.75      0.82       236\n",
            "           3       0.80      0.84      0.82       195\n",
            "           4       0.79      0.90      0.85       188\n",
            "           5       0.84      0.74      0.79       190\n",
            "           6       0.88      0.91      0.90       176\n",
            "           7       0.81      0.85      0.83       215\n",
            "           8       0.80      0.72      0.76       205\n",
            "           9       0.82      0.71      0.76       201\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.84      0.84      0.83      2000\n",
            "weighted avg       0.84      0.83      0.83      2000\n",
            "\n",
            "The Label with the lowest precision is 4\n"
          ]
        }
      ]
    }
  ]
}